{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run setup code\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Conv1D, Dropout, GRU, Bidirectional, Conv2D\n",
    "from keras.layers import Reshape, Activation, Flatten, TimeDistributed,MaxPooling1D, MaxPooling2D\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers.merge import Dot\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = '/data/wongnai-review/'\n",
    "data_raw = pd.read_csv(path + 'w_review_train.csv', delimiter=';', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ร้านอาหารใหญ่มากกกกกกก \\nเลี้ยวเข้ามาเจอห้องน้...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>อาหารที่นี่เป็นอาหารจีนแคะที่หากินยากในบ้านเรา...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ปอเปี๊ยะสด ทุกวันนี้รู้สึกว่าหากินยาก (ร้านที่...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>รัานคัพเค้กในเมืองไทยมีไม่มาก หลายๆคนอาจจะสงสั...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>อร่อย!!! เดินผ่านDigital gatewayทุกวัน ไม่ยักร...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sent  rating\n",
       "0  ร้านอาหารใหญ่มากกกกกกก \\nเลี้ยวเข้ามาเจอห้องน้...       3\n",
       "1  อาหารที่นี่เป็นอาหารจีนแคะที่หากินยากในบ้านเรา...       4\n",
       "2  ปอเปี๊ยะสด ทุกวันนี้รู้สึกว่าหากินยาก (ร้านที่...       3\n",
       "3  รัานคัพเค้กในเมืองไทยมีไม่มาก หลายๆคนอาจจะสงสั...       5\n",
       "4  อร่อย!!! เดินผ่านDigital gatewayทุกวัน ไม่ยักร...       5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw = pd.DataFrame(data=data_raw)\n",
    "data_raw = data_raw.rename(index=str, columns={0: \"sent\", 1: \"rating\"})\n",
    "data_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "data_raw['sent'] = data_raw['sent'].apply(lambda k : re.sub(r'[\"|–|\\'|:|;|?|$|!|~|\\n|\\t|-|#|+|<|>|/|\\\\|\\|{|}|\\[|\\]|`|0|1|2|3|4|5|6|7|8|9|*|.|%|@|$|^|&|=|:|(|)|-|_]', r'', k))\n",
    "# data_raw['sent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ร, ้, า, น, อ, า, ห, า, ร, ใ, ห, ญ, ่, ม, า, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[อ, า, ห, า, ร, ท, ี, ่, น, ี, ่, เ, ป, ็, น, ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ป, อ, เ, ป, ี, ๊, ย, ะ, ส, ด,  , ท, ุ, ก, ว, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[ร, ั, า, น, ค, ั, พ, เ, ค, ้, ก, ใ, น, เ, ม, ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[อ, ร, ่, อ, ย,  , เ, ด, ิ, น, ผ, ่, า, น, D, ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sent  rating\n",
       "0  [ร, ้, า, น, อ, า, ห, า, ร, ใ, ห, ญ, ่, ม, า, ...       3\n",
       "1  [อ, า, ห, า, ร, ท, ี, ่, น, ี, ่, เ, ป, ็, น, ...       4\n",
       "2  [ป, อ, เ, ป, ี, ๊, ย, ะ, ส, ด,  , ท, ุ, ก, ว, ...       3\n",
       "3  [ร, ั, า, น, ค, ั, พ, เ, ค, ้, ก, ใ, น, เ, ม, ...       5\n",
       "4  [อ, ร, ่, อ, ย,  , เ, ด, ิ, น, ผ, ่, า, น, D, ...       5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set = data_raw.copy()\n",
    "data_set['sent'] = data_raw['sent'].apply(lambda row: list(row))\n",
    "data_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a character map\n",
    "CHARS = [\n",
    "  '\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+',\n",
    "  ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8',\n",
    "  '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E',\n",
    "  'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R',\n",
    "  'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_',\n",
    "  'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
    "  'n', 'o', 'other', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y',\n",
    "  'z', '}', '~', 'ก', 'ข', 'ฃ', 'ค', 'ฅ', 'ฆ', 'ง', 'จ', 'ฉ', 'ช',\n",
    "  'ซ', 'ฌ', 'ญ', 'ฎ', 'ฏ', 'ฐ', 'ฑ', 'ฒ', 'ณ', 'ด', 'ต', 'ถ', 'ท',\n",
    "  'ธ', 'น', 'บ', 'ป', 'ผ', 'ฝ', 'พ', 'ฟ', 'ภ', 'ม', 'ย', 'ร', 'ฤ',\n",
    "  'ล', 'ว', 'ศ', 'ษ', 'ส', 'ห', 'ฬ', 'อ', 'ฮ', 'ฯ', 'ะ', 'ั', 'า',\n",
    "  'ำ', 'ิ', 'ี', 'ึ', 'ื', 'ุ', 'ู', 'ฺ', 'เ', 'แ', 'โ', 'ใ', 'ไ',\n",
    "  'ๅ', 'ๆ', '็', '่', '้', '๊', '๋', '์', 'ํ', '๐', '๑', '๒', '๓',\n",
    "  '๔', '๕', '๖', '๗', '๘', '๙', '‘', '’', '\\ufeff'\n",
    "]\n",
    "CHARS_MAP = {v: k for k, v in enumerate(CHARS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_n_gram_df(df, n_pad):\n",
    "    \"\"\"\n",
    "    Given an input dataframe, create a feature dataframe of shifted characters\n",
    "    Input:\n",
    "    df: timeseries of size (N)\n",
    "    n_pad: the number of context. For a given character at position [idx],\n",
    "    character at position [idx-n_pad/2 : idx+n_pad/2] will be used \n",
    "    as features for that character.\n",
    "\n",
    "    Output:\n",
    "    dataframe of size (N * n_pad) which each row contains the character, \n",
    "    n_pad_2 characters to the left, and n_pad_2 characters to the right\n",
    "    of that character.\n",
    "    \"\"\"\n",
    "    n_pad_2 = int((n_pad - 1)/2)\n",
    "    for i in range(n_pad_2):\n",
    "        df['char-{}'.format(i+1)] = df['char'].shift(i + 1)\n",
    "        df['char{}'.format(i+1)] = df['char'].shift(-i - 1)\n",
    "    return df[n_pad_2: -n_pad_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_feature(input_string):\n",
    "    \"\"\"\n",
    "    Transform the path to a directory containing processed files \n",
    "    into a feature matrix and output array\n",
    "    Input:\n",
    "    best_processed_path: str, path to a processed version of the BEST dataset\n",
    "    option: str, 'train' or 'test'\n",
    "    \"\"\"\n",
    "    # we use padding equals 21 here to consider 10 characters to the left\n",
    "    # and 10 characters to the right as features for the character in the middle\n",
    "    n_pad = 21\n",
    "    n_pad_2 = int((n_pad - 1)/2)\n",
    "    pad = [{'char' : ' '}]\n",
    "    df_pad = pd.DataFrame(pad * n_pad_2)\n",
    "\n",
    "#     df = pd.DataFrame(data=best_processed_path['sent'][0], columns=['char'])\n",
    "    df = pd.DataFrame(data=input_string, columns=['char'])\n",
    "    # pad with empty string feature\n",
    "    df = pd.concat((df_pad, df, df_pad))\n",
    "    \n",
    "    # map characters to numbers, use 'other' if not in the predefined character set.\n",
    "    df['char'] = df['char'].map(lambda x: CHARS_MAP.get(x, 80))\n",
    "    # Use nearby characters as features\n",
    "    df_with_context = create_n_gram_df(df, n_pad=n_pad)\n",
    "\n",
    "    char_row = ['char' + str(i + 1) for i in range(n_pad_2)] + \\\n",
    "             ['char-' + str(i + 1) for i in range(n_pad_2)] + ['char']\n",
    "\n",
    "    # convert pandas dataframe to numpy array to feed to the model\n",
    "    x_char = df_with_context[char_row].as_matrix()\n",
    "\n",
    "    return x_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print char of feature 1\n",
    "char = np.array(CHARS)\n",
    "\n",
    "#A function for displaying our features in text\n",
    "def print_features(tfeature,label,index):\n",
    "    feature = np.array(tfeature[index],dtype=int).reshape(21,1)\n",
    "    #Convert to string\n",
    "    char_list = char[feature]\n",
    "    left = ''.join(reversed(char_list[10:20].reshape(10))).replace(\" \", \"\")\n",
    "    center = ''.join(char_list[20])\n",
    "    right =  ''.join(char_list[0:10].reshape(10)).replace(\" \", \"\")\n",
    "    word = ''.join([left,' ',center,' ',right])\n",
    "    print(center + ': ' + word + \"\\tpred = \"+str(label[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenize model\n",
    "def get_my_tokenize_model():\n",
    "    input1 = Input(shape=(21,))\n",
    "    x = Embedding(178,8)(input1)\n",
    "    x = Conv1D(100,5,strides=1,activation='relu',padding=\"same\")(x)\n",
    "    x = TimeDistributed(Dense(5))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    out = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs=input1, outputs=out)\n",
    "    model.compile(optimizer=Adam(),\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['acc'])          \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##progress bar\n",
    "import progressbar\n",
    "def set_progressbar(l):\n",
    "    return progressbar.ProgressBar(maxval=l, \\\n",
    "    widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## LOAD Tokenize model\n",
    "weight_path_model_best='/data/model_best.h5'\n",
    "\n",
    "tokenize_model = get_my_tokenize_model()\n",
    "tokenize_model.load_weights(weight_path_model_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def map_pred_to_word(y_pred,sent):\n",
    "    out = []\n",
    "    w = ''\n",
    "    for i in range(len(y_pred)):\n",
    "        if(y_pred[i] == 1):\n",
    "            out.append(w)\n",
    "            w = sent[i]\n",
    "        else:\n",
    "            w += sent[i]\n",
    "#         t = w.strip()\n",
    "#         if(t != ''):\n",
    "    out.append(w.strip())\n",
    "    return out[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [25:21<00:00, 26.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31min 39s, sys: 3min 3s, total: 34min 43s\n",
      "Wall time: 25min 21s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tqdm import tqdm\n",
    "# predict\n",
    "# bar = set_progressbar(len(data_set))\n",
    "# bar.start()\n",
    "\n",
    "tokenized_sent = []\n",
    "for i in tqdm(range(len(data_set))):\n",
    "    chars_array = prepare_feature(data_set['sent'][i])\n",
    "    y_pred = tokenize_model.predict(chars_array)\n",
    "    #map probability to class\n",
    "    prob_to_class = lambda p: 1 if p[0]>=0.5 else 0\n",
    "    y_pred = np.apply_along_axis(prob_to_class,1,y_pred)\n",
    "    tokenized_sent.append(map_pred_to_word(y_pred,data_raw['sent'][i]))\n",
    "#     bar.update(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = np.array(tokenized_sent)\n",
    "y_train = data_set['rating'].as_matrix()\n",
    "prepared_data = pd.DataFrame(data={'sent':X_train, 'rating':y_train},columns=['sent','rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save tokenize\n",
    "with open('/data/tokenized-review-clean', 'wb') as f:\n",
    "    pickle.dump(prepared_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load tokenize\n",
    "with open('/data/tokenized-review-clean', 'rb') as f:\n",
    "    tokenized_sent = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tokenized_sent['sent'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## read fasttext\n",
    "ftext_w = {}\n",
    "with open('/data/fasttext/wiki.th.vec', 'r') as f:\n",
    "    embeded_w = f.readlines()\n",
    "for line in embeded_w:\n",
    "    values = line.rstrip().rsplit(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    ftext_w[word] = coefs\n",
    "    \n",
    "# word_to_idx = {}\n",
    "# idx_to_word = ['for_keras_zero_padding']\n",
    "\n",
    "# for w in ftext_w.keys():\n",
    "#     word_to_idx[w] = len(idx_to_word)\n",
    "#     idx_to_word.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51359it [00:06, 7590.56it/s]\n"
     ]
    }
   ],
   "source": [
    "## read thai2vec\n",
    "thai2vec = dict()\n",
    "with open('/data/thai2vec.vec', 'r') as f:\n",
    "    for i, line in tqdm(enumerate(f), ncols=10):\n",
    "        if(i == 0) :\n",
    "            continue\n",
    "        thai2vec[line.split(' ')[0]] = [np.float(x) for x in line.split(' ')[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51358it [00:00, 541723.83it/s]\n"
     ]
    }
   ],
   "source": [
    "word_to_idx = {}\n",
    "with open('/data/thai2vec.vocab', 'r') as f:\n",
    "    for i, line in tqdm(enumerate(f), ncols=10):\n",
    "        word_to_idx[line.split(' ')[0]] = int(line.split(' ')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(word_to_idx), len(thai2vec))\n",
    "print(thai2vec['UNK'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172055 0.02664951523454532\n",
      "[('', 1115292), ('UNK', 172055), ('ๆ', 112143), ('ร้าน', 110083), ('ที่', 108221), ('ไม่', 101023), ('มา', 92063), ('มี', 81201), ('นี้', 72266), ('ได้', 70890)]\n"
     ]
    }
   ],
   "source": [
    "max_len = 1000\n",
    "def create_index(input_text):\n",
    "    count_word = 0\n",
    "    words = []\n",
    "    \n",
    "    for sent in input_text:\n",
    "        for w in sent:\n",
    "            words.append(w.strip('\\n'))\n",
    "            count_word +=1\n",
    "    \n",
    "    word_count = list()\n",
    "    #use set and len to get the number of unique words\n",
    "    word_count.extend(collections.Counter(words).most_common(len(set(words))))\n",
    "    \n",
    "    #include a token for unknown word\n",
    "    threshold = 5\n",
    "    num_UNK = 0\n",
    "    index = len(word_count) - 1\n",
    "    rare_word = set()\n",
    "    \n",
    "    \n",
    "    while(word_count[index][1] <= threshold):\n",
    "        num_UNK += word_count[index][1]\n",
    "        rare_word.add(word_count[index][0])\n",
    "        index -= 1\n",
    "    \n",
    "    word_count = word_count[:index+1]\n",
    "    word_count.append((\"UNK\",num_UNK))\n",
    "    word_count = sorted(word_count, key=lambda x: -x[1])\n",
    "    \n",
    "    print(num_UNK , num_UNK/count_word)       \n",
    "\n",
    "    #print out 10 most frequent words\n",
    "    \n",
    "    print(word_count[:10])\n",
    "    dictionary = dict()\n",
    "    dictionary[\"for_keras_zero_padding\"] = 0\n",
    "    \n",
    "    for word in word_count:\n",
    "        dictionary[word[0]] = len(dictionary)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    \n",
    "    dataset = list()\n",
    "    for sent in input_text:\n",
    "        dataset.append([])\n",
    "        for word in sent[:max_len]:\n",
    "            if(word not in rare_word):\n",
    "                dataset[-1].append(dictionary[word])\n",
    "            else:\n",
    "                dataset[-1].append(dictionary[\"UNK\"])\n",
    "\n",
    "    return dataset, dictionary, reverse_dictionary\n",
    "\n",
    "dataset ,dictionary,reverse_dictionary = create_index(tokenized_sent['sent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_len = 500\n",
    "def create_index(input_text):\n",
    "    count_word = 0\n",
    "    words = []\n",
    "    \n",
    "    for sent in input_text:\n",
    "        for w in sent[:max_len]:\n",
    "            words.append(w.strip('\\n'))\n",
    "            count_word +=1\n",
    "    \n",
    "    word_count = list()\n",
    "    #use set and len to get the number of unique words\n",
    "    word_count.extend(collections.Counter(words).most_common(len(set(words))))\n",
    "    \n",
    "    #include a token for unknown word\n",
    "    threshold = 10\n",
    "    num_UNK = 0\n",
    "    index = len(word_count) - 1\n",
    "    rare_word = set()\n",
    "    \n",
    "    \n",
    "    while(word_count[index][1] <= threshold):\n",
    "        num_UNK += word_count[index][1]\n",
    "        rare_word.add(word_count[index][0])\n",
    "        index -= 1\n",
    "    \n",
    "    word_count = word_count[:index+1]\n",
    "    word_count.append((\"UNK\",num_UNK))\n",
    "    word_count = sorted(word_count, key=lambda x: -x[1])\n",
    "    \n",
    "    print(num_UNK , num_UNK/count_word)       \n",
    "\n",
    "    #print out 10 most frequent words\n",
    "    \n",
    "    print(word_count[:10])\n",
    "    dictionary = word_to_idx.copy()\n",
    "    dictionary[\"for_keras_zero_padding\"] = 0\n",
    "        \n",
    "    for word in word_count:\n",
    "        word = word[0].strip('\\n')\n",
    "        if(not word in word_to_idx.keys()):\n",
    "            word_to_idx[word] = len(idx_to_word)\n",
    "            dictionary[word] = len(idx_to_word)\n",
    "            idx_to_word.append(word)\n",
    "        else:\n",
    "            dictionary[word] = word_to_idx[word]\n",
    "            \n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))    \n",
    "    \n",
    "    dataset = list()\n",
    "    for sent in input_text:\n",
    "        dataset.append([])\n",
    "        for word in sent[:max_len]:\n",
    "            word = word.strip('\\n')\n",
    "            if(word not in rare_word):\n",
    "                dataset[-1].append(dictionary[word])\n",
    "            else:\n",
    "                dataset[-1].append(dictionary[\"UNK\"])\n",
    "\n",
    "    return dataset, dictionary, reverse_dictionary\n",
    "\n",
    "dataset ,dictionary,reverse_dictionary = create_index(tokenized_sent['sent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = sequence.pad_sequences(dataset, maxlen=max_len, padding='post', truncating='pre') #padding\n",
    "y_train = data_set['rating'].as_matrix()\n",
    "\n",
    "\n",
    "# y_test = y_train[int(len(y_train)*0.9):]\n",
    "# y_train = y_train[:int(len(y_train)*0.9)]\n",
    "y_train = pd.get_dummies(y_train).as_matrix()\n",
    "\n",
    "# X_test = X_train[int(len(X_train)*0.9):]\n",
    "# X_train = X_train[:int(len(X_train)*0.9)]\n",
    "# X_dev = X_train[int(len(X_train)*0.7):int(len(X_train)*0.9)]\n",
    "\n",
    "\n",
    "# print(len(X_train), len(y_train), len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match 7013 0.4218345864661654\n"
     ]
    }
   ],
   "source": [
    "match = 0\n",
    "for w in dictionary.keys():\n",
    "    if(w.strip('\\n') in ftext_w.keys()):\n",
    "        match += 1\n",
    "\n",
    "print('match', match, match/len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Prepare embed layer\n",
    "def prepare_embed(pretrain):\n",
    "    pre_emb = []\n",
    "    pre_emb.append(np.zeros(300))\n",
    "\n",
    "    for k in dictionary.keys():\n",
    "        if(not k in pretrain.keys()):    \n",
    "            pre_emb.append(np.zeros(300))\n",
    "        else:\n",
    "            if(len(pretrain[k]) ==  300):\n",
    "                pre_emb.append(pretrain[k])\n",
    "            else:\n",
    "                pre_emb.append(np.zeros(300))\n",
    "    return pre_emb\n",
    "\n",
    "pre_emb = prepare_embed(ftext_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(pre_emb[68143])\n",
    "pre_emb_w = np.array(pre_emb.copy())\n",
    "# pre_emb_w[20591] = np.array(ftext_w[idx_to_word[20591]])\n",
    "# print(pre_emb_w[67344])\n",
    "has_no_300 = 0\n",
    "for i,e in enumerate(pre_emb_w):\n",
    "    if (len(e) != 300):\n",
    "        has_no_300 += 1\n",
    "        print(i, len(e), e)\n",
    "#         pre_emb_w[i] = ftext_w[idx_to_word[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_10 (Embedding)     (None, 1000, 300)         4987800   \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1000, 64)          192064    \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 1000, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (None, 1000, 5)           325       \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 5000)              0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 100)               500100    \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 5,711,438\n",
      "Trainable params: 5,711,438\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Predict Model\n",
    "def get_predict_model():\n",
    "    input1 = Input(shape=(max_len,))\n",
    "    x = Embedding(len(dictionary)+1, 300, weights=[pre_emb_w], trainable=True)(input1)\n",
    "    x = Conv1D(64,10,strides=1,activation='relu',padding=\"same\")(x)\n",
    "#     x = MaxPooling1D(pool_size=4, strides=1, padding='same')(x)    \n",
    "    x = Conv1D(64,5,strides=1,activation='relu',padding=\"same\")(x)\n",
    "#     x = Conv1D(16,2,strides=1,activation='relu',padding=\"same\")(x)\n",
    "    x = MaxPooling1D(pool_size=5, strides=1, padding='same')(x)   \n",
    "    x = Dropout(0.5)(x)\n",
    "    x = TimeDistributed(Dense(5))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    out = Dense(5, activation='softmax')(x)\n",
    "    model = Model(inputs=input1, outputs=out)\n",
    "    model.compile(optimizer=Adam(),\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['categorical_accuracy'])          \n",
    "    return model\n",
    "model = get_predict_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/3\n",
      "31744/32000 [============================>.] - ETA: 0s - loss: 1.2542 - categorical_accuracy: 0.4475Epoch 00000: val_loss improved from inf to 1.19041, saving model to /data/midterm-1.h5\n",
      "32000/32000 [==============================] - 41s - loss: 1.2543 - categorical_accuracy: 0.4473 - val_loss: 1.1904 - val_categorical_accuracy: 0.4691\n",
      "Epoch 2/3\n",
      "31744/32000 [============================>.] - ETA: 0s - loss: 1.1729 - categorical_accuracy: 0.4792Epoch 00001: val_loss improved from 1.19041 to 1.09861, saving model to /data/midterm-1.h5\n",
      "32000/32000 [==============================] - 39s - loss: 1.1727 - categorical_accuracy: 0.4794 - val_loss: 1.0986 - val_categorical_accuracy: 0.5108\n",
      "Epoch 3/3\n",
      "31744/32000 [============================>.] - ETA: 0s - loss: 1.0351 - categorical_accuracy: 0.5372Epoch 00002: val_loss improved from 1.09861 to 1.02520, saving model to /data/midterm-1.h5\n",
      "32000/32000 [==============================] - 39s - loss: 1.0350 - categorical_accuracy: 0.5371 - val_loss: 1.0252 - val_categorical_accuracy: 0.5346\n",
      "CPU times: user 1min 5s, sys: 25.1 s, total: 1min 30s\n",
      "Wall time: 2min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "weight_path_model_best='/data/midterm-1.h5'\n",
    "\n",
    "callbacks_list = [\n",
    "#    TensorBoard(log_dir='/data/Graph/midterm', histogram_freq=1, write_grads=True),\n",
    "    ModelCheckpoint(\n",
    "        weight_path_model_best,\n",
    "        monitor = \"val_loss\",\n",
    "        mode = 'min',\n",
    "        verbose = 1,\n",
    "        save_best_only = True,\n",
    "        save_weights_only = True,\n",
    "    ),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                    patience=2, min_lr=0.001)\n",
    "]\n",
    "\n",
    "model.fit(X_train,y_train,batch_size=256,epochs=3,verbose=1, validation_split=0.2, shuffle=True, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/3\n",
      "31744/32000 [============================>.] - ETA: 0s - loss: 0.8990 - categorical_accuracy: 0.5957Epoch 00000: val_loss did not improve\n",
      "32000/32000 [==============================] - 39s - loss: 0.8991 - categorical_accuracy: 0.5957 - val_loss: 1.0374 - val_categorical_accuracy: 0.5424\n",
      "Epoch 2/3\n",
      "21248/32000 [==================>...........] - ETA: 12s - loss: 0.7494 - categorical_accuracy: 0.6767"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-c270810cb63b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/ekapolc/.env/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1598\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1599\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[1;32m/home/ekapolc/.env/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ekapolc/.env/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2273\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2274\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ekapolc/.env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ekapolc/.env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1124\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1125\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ekapolc/.env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1321\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ekapolc/.env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ekapolc/.env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train,y_train,batch_size=256,epochs=3,verbose=1, validation_split=0.2, shuffle=True, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(weight_path_model_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_58 (InputLayer)        (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_58 (Embedding)     (None, 1000, 300)         6978300   \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, 996, 64)           96064     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_79 (MaxPooling (None, 992, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 988, 32)           10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_80 (MaxPooling (None, 984, 32)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_41 (TimeDis (None, 984, 5)            165       \n",
      "_________________________________________________________________\n",
      "flatten_51 (Flatten)         (None, 4920)              0         \n",
      "_________________________________________________________________\n",
      "dense_165 (Dense)            (None, 100)               492100    \n",
      "_________________________________________________________________\n",
      "dense_166 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_167 (Dense)            (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 7,587,506\n",
      "Trainable params: 7,587,506\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Predict Model\n",
    "# import keras.backend as K\n",
    "# K.clear_session()\n",
    "def get_predict_model2():\n",
    "    input1 = Input(shape=(max_len,))\n",
    "    x = Embedding(len(dictionary)+1, 300, weights=[pre_emb_w], trainable=True)(input1)\n",
    "    x = Conv1D(64,5,strides=1,activation='relu',padding=\"valid\")(x)\n",
    "    x = MaxPooling1D(pool_size=5, strides=1, padding='valid')(x)    \n",
    "    x = Conv1D(32,5,strides=1,activation='relu',padding=\"valid\")(x)\n",
    "    x = MaxPooling1D(pool_size=5, strides=1, padding='valid')(x)    \n",
    "    x = Dropout(0.25)(x)\n",
    "    x = TimeDistributed(Dense(5))(x)\n",
    "    x = Flatten()(x)\n",
    "#     x = Dense(100, activation='relu')(x)\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    out = Dense(5, activation='softmax')(x)\n",
    "    model = Model(inputs=input1, outputs=out)\n",
    "    model.compile(optimizer=Adam(lr=0.001),\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['categorical_accuracy'])          \n",
    "    return model\n",
    "model2 = get_predict_model2()\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28800 samples, validate on 7200 samples\n",
      "Epoch 1/5\n",
      "28672/28800 [============================>.] - ETA: 0s - loss: 1.2346 - categorical_accuracy: 0.4602Epoch 00000: val_loss improved from inf to 1.20696, saving model to /data/midterm-2.h5\n",
      "28800/28800 [==============================] - 25s - loss: 1.2349 - categorical_accuracy: 0.4599 - val_loss: 1.2070 - val_categorical_accuracy: 0.4647\n",
      "Epoch 2/5\n",
      "28672/28800 [============================>.] - ETA: 0s - loss: 1.1840 - categorical_accuracy: 0.4699Epoch 00001: val_loss improved from 1.20696 to 1.19126, saving model to /data/midterm-2.h5\n",
      "28800/28800 [==============================] - 23s - loss: 1.1840 - categorical_accuracy: 0.4702 - val_loss: 1.1913 - val_categorical_accuracy: 0.4690\n",
      "Epoch 3/5\n",
      "28672/28800 [============================>.] - ETA: 0s - loss: 1.0440 - categorical_accuracy: 0.5233Epoch 00002: val_loss improved from 1.19126 to 1.04297, saving model to /data/midterm-2.h5\n",
      "28800/28800 [==============================] - 23s - loss: 1.0439 - categorical_accuracy: 0.5233 - val_loss: 1.0430 - val_categorical_accuracy: 0.5164\n",
      "Epoch 4/5\n",
      "28672/28800 [============================>.] - ETA: 0s - loss: 0.9006 - categorical_accuracy: 0.5964Epoch 00003: val_loss improved from 1.04297 to 1.03294, saving model to /data/midterm-2.h5\n",
      "28800/28800 [==============================] - 23s - loss: 0.9003 - categorical_accuracy: 0.5965 - val_loss: 1.0329 - val_categorical_accuracy: 0.5329\n",
      "Epoch 5/5\n",
      "28672/28800 [============================>.] - ETA: 0s - loss: 0.7484 - categorical_accuracy: 0.6741Epoch 00004: val_loss did not improve\n",
      "28800/28800 [==============================] - 23s - loss: 0.7486 - categorical_accuracy: 0.6740 - val_loss: 1.0882 - val_categorical_accuracy: 0.5221\n",
      "CPU times: user 1min 1s, sys: 24.6 s, total: 1min 26s\n",
      "Wall time: 1min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# use 1000, \n",
    "weight_path_model_best2='/data/midterm-2.h5'\n",
    "\n",
    "callbacks_list = [\n",
    "#    TensorBoard(log_dir='/data/Graph/midterm', histogram_freq=1, write_grads=True),\n",
    "    ModelCheckpoint(\n",
    "        weight_path_model_best2,\n",
    "        monitor = \"val_loss\",\n",
    "        mode = 'min',\n",
    "        verbose = 1,\n",
    "        save_best_only = True,\n",
    "        save_weights_only = True,\n",
    "    ),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                    patience=2, min_lr=0.001)\n",
    "]\n",
    "\n",
    "model2.fit(X_train,y_train,batch_size=512,epochs=5,verbose=1, validation_split=0.2, shuffle=True, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 1000, 300)         6978300   \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 996, 64)           96064     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 992, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 992, 64)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 992, 5)            325       \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 4960)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 100)               496100    \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 7,581,394\n",
      "Trainable params: 7,581,394\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Predict Model\n",
    "### BEST NOW\n",
    "### first 1000, threshold 3, thai2vec\n",
    "def get_predict_model3():\n",
    "    input1 = Input(shape=(max_len,))\n",
    "    x = Embedding(len(dictionary)+1, 300, weights=[pre_emb_w], trainable=True)(input1)\n",
    "#     x = Conv1D(32,5,strides=1,activation='relu',padding=\"valid\")(x)\n",
    "#     x = MaxPooling1D(pool_size=5, strides=1,padding='valid')(x)    \n",
    "    x = Conv1D(64,5,strides=1,activation='relu',padding=\"valid\")(x)\n",
    "    x = MaxPooling1D(pool_size=5, strides=1,padding='valid')(x)    \n",
    "    x = Dropout(0.25)(x)\n",
    "    x = TimeDistributed(Dense(5))(x)\n",
    "    x = Flatten()(x)\n",
    "#     x = Dense(100, activation='relu')(x)\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    out = Dense(5, activation='softmax')(x)\n",
    "    model = Model(inputs=input1, outputs=out)\n",
    "    model.compile(optimizer=Adam(lr=0.001),\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['categorical_accuracy'])          \n",
    "    return model\n",
    "model3 = get_predict_model3()\n",
    "model3.summary()\n",
    "weight_path_model_best3='/data/midterm-3.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/5\n",
      "31744/32000 [============================>.] - ETA: 0s - loss: 1.2389 - categorical_accuracy: 0.4526Epoch 00000: val_loss improved from inf to 1.14091, saving model to /data/midterm-3.h5\n",
      "32000/32000 [==============================] - 26s - loss: 1.2384 - categorical_accuracy: 0.4526 - val_loss: 1.1409 - val_categorical_accuracy: 0.4798\n",
      "Epoch 2/5\n",
      "31744/32000 [============================>.] - ETA: 0s - loss: 1.0756 - categorical_accuracy: 0.5170Epoch 00001: val_loss improved from 1.14091 to 1.00867, saving model to /data/midterm-3.h5\n",
      "32000/32000 [==============================] - 24s - loss: 1.0755 - categorical_accuracy: 0.5171 - val_loss: 1.0087 - val_categorical_accuracy: 0.5415\n",
      "Epoch 3/5\n",
      "31744/32000 [============================>.] - ETA: 0s - loss: 0.9345 - categorical_accuracy: 0.5805Epoch 00002: val_loss did not improve\n",
      "32000/32000 [==============================] - 24s - loss: 0.9338 - categorical_accuracy: 0.5809 - val_loss: 1.0498 - val_categorical_accuracy: 0.5415\n",
      "Epoch 4/5\n",
      "31744/32000 [============================>.] - ETA: 0s - loss: 0.7977 - categorical_accuracy: 0.6508Epoch 00003: val_loss did not improve\n",
      "32000/32000 [==============================] - 24s - loss: 0.7979 - categorical_accuracy: 0.6508 - val_loss: 1.0913 - val_categorical_accuracy: 0.5050\n",
      "Epoch 5/5\n",
      "31744/32000 [============================>.] - ETA: 0s - loss: 0.6068 - categorical_accuracy: 0.7486Epoch 00004: val_loss did not improve\n",
      "32000/32000 [==============================] - 24s - loss: 0.6067 - categorical_accuracy: 0.7485 - val_loss: 1.3240 - val_categorical_accuracy: 0.5104\n",
      "CPU times: user 1min 7s, sys: 26.1 s, total: 1min 33s\n",
      "Wall time: 2min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "callbacks_list = [\n",
    "#    TensorBoard(log_dir='/data/Graph/midterm', histogram_freq=1, write_grads=True),\n",
    "    ModelCheckpoint(\n",
    "        weight_path_model_best3,\n",
    "        monitor = \"val_loss\",\n",
    "        mode = 'min',\n",
    "        verbose = 1,\n",
    "        save_best_only = True,\n",
    "        save_weights_only = True,\n",
    "    ),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                    patience=2, min_lr=0.001)\n",
    "]\n",
    "\n",
    "# model3.fit(X_train,y_train,batch_size=256,epochs=5,verbose=1, validation_split=0.2, shuffle=True, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model3.load_weights(weight_path_model_best3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score,precision_score,recall_score\n",
    "def evaluate(x_test, y_test, model):\n",
    "    \"\"\"\n",
    "    Evaluate model on the splitted 10 percent testing set.\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(x_test)\n",
    "    #map probability to class\n",
    "    y_pred_mapped = []\n",
    "    for i,pred in enumerate(y_pred):\n",
    "        pred = list(pred)\n",
    "        y_pred_mapped.append(pred.index(max(pred))+1)    \n",
    "    \n",
    "    f1score = f1_score(y_test,y_pred_mapped, average='weighted')\n",
    "    precision = precision_score(y_test,y_pred_mapped, average='weighted')\n",
    "    recall = recall_score(y_test,y_pred_mapped, average='weighted')\n",
    "    return f1score, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000 4000\n"
     ]
    }
   ],
   "source": [
    "print(len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4794448217436239, 0.5883356096010524, 0.5465)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewID</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ร้านนี้จะอยู่เส้นสันกำแพง-แม่ออน เลยแยกบ่...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>สั่งไป เมนู คือมัชฉะลาเต้ร้อน กับ ไอศครีมชาเขี...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>ครัววงเดือน  หิวดึกๆ ตระเวนหาร้านทาน มาเจอร้าน...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>จะว่าเป็นเจ้าประจำก็คงไม่ผิด แต่ก็ไม่กล้า...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>ถ้าคิดถึงสลัดผมคิดถึงร้านนี้เป็นร้านแรกๆเลยครั...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reviewID                                             review\n",
       "0         1       ร้านนี้จะอยู่เส้นสันกำแพง-แม่ออน เลยแยกบ่...\n",
       "1         2  สั่งไป เมนู คือมัชฉะลาเต้ร้อน กับ ไอศครีมชาเขี...\n",
       "2         3  ครัววงเดือน  หิวดึกๆ ตระเวนหาร้านทาน มาเจอร้าน...\n",
       "3         4       จะว่าเป็นเจ้าประจำก็คงไม่ผิด แต่ก็ไม่กล้า...\n",
       "4         5  ถ้าคิดถึงสลัดผมคิดถึงร้านนี้เป็นร้านแรกๆเลยครั..."
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## PREDICT\n",
    "test_raw = pd.read_csv(path + 'test_file.csv', delimiter=';', header=0)\n",
    "test_raw = pd.DataFrame(data=test_raw, columns=['reviewID','review'])\n",
    "test_raw['review'] = test_raw['review'].apply(lambda k : re.sub(r'[\"|–|\\'|:|;|?|$|!|~|\\n|\\t|-|#|+|<|>|/|\\\\|\\|{|}|\\[|\\]|`|0|1|2|3|4|5|6|7|8|9|*|.|%|@|$|^|&|=|:|(|)|-|_]', r'', k))\n",
    "test_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewID</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[ ,  ,  ,  ,  , ร, ้, า, น, น, ี, ้, จ, ะ, อ, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[ส, ั, ่, ง, ไ, ป,  , เ, ม, น, ู,  , ค, ื, อ, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[ค, ร, ั, ว, ว, ง, เ, ด, ื, อ, น,  ,  , ห, ิ, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[ ,  ,  ,  ,  , จ, ะ, ว, ่, า, เ, ป, ็, น, เ, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[ถ, ้, า, ค, ิ, ด, ถ, ึ, ง, ส, ล, ั, ด, ผ, ม, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reviewID                                             review\n",
       "0         1  [ ,  ,  ,  ,  , ร, ้, า, น, น, ี, ้, จ, ะ, อ, ...\n",
       "1         2  [ส, ั, ่, ง, ไ, ป,  , เ, ม, น, ู,  , ค, ื, อ, ...\n",
       "2         3  [ค, ร, ั, ว, ว, ง, เ, ด, ื, อ, น,  ,  , ห, ิ, ...\n",
       "3         4  [ ,  ,  ,  ,  , จ, ะ, ว, ่, า, เ, ป, ็, น, เ, ...\n",
       "4         5  [ถ, ้, า, ค, ิ, ด, ถ, ึ, ง, ส, ล, ั, ด, ผ, ม, ..."
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "test_list = test_raw.copy()\n",
    "test_list['review'] = test_raw['review'].apply(lambda row: list(row))\n",
    "test_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tokenized_test = []\n",
    "\n",
    "for i in tqdm(range(len(test_list)), ncols=100):\n",
    "    chars_array = prepare_feature(test_list['review'][i])\n",
    "    y_pred = tokenize_model.predict(chars_array)\n",
    "    #map probability to class\n",
    "    prob_to_class = lambda p:1 if p[0]>=0.5 else 0\n",
    "    y_pred = np.apply_along_axis(prob_to_class,1,y_pred)\n",
    "    tokenized_test.append(map_pred_to_word(y_pred,test_raw['review'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open ('/data/test_tokened','wb') as f:\n",
    "    pickle.dump(tokenized_test,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open ('/data/test_tokened','rb') as f:\n",
    "    tokenized_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████| 6203/6203 [00:00<00:00, 10375.63it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# data['review'] = test_raw['review'].apply(lambda row: list(row))\n",
    "dataset_test = list()\n",
    "for i in tqdm(range(len(tokenized_test)), ncols=100):\n",
    "    sent = tokenized_test[i]\n",
    "    dataset_test.append([])\n",
    "    for word in sent[:max_len]:\n",
    "        word = word.strip('\\n')\n",
    "        if(word in dictionary.keys()):\n",
    "            dataset_test[-1].append(dictionary[word])\n",
    "        else:\n",
    "            dataset_test[-1].append(dictionary[\"UNK\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.06 s, sys: 210 ms, total: 1.27 s\n",
      "Wall time: 2.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_prep = sequence.pad_sequences(dataset_test, maxlen=max_len, padding='post', truncating='pre')\n",
    "test_pred = model.predict(test_prep)\n",
    "ans = []\n",
    "for i,pred in enumerate(test_pred):\n",
    "    pred = list(pred)\n",
    "    mapped = pred.index(max(pred))+1\n",
    "    ans.append(mapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>reviewID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating  reviewID\n",
       "0       4         1\n",
       "1       3         2\n",
       "2       4         3\n",
       "3       4         4\n",
       "4       4         5"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = pd.DataFrame({'reviewID':test_raw['reviewID'],'rating':ans})\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output.to_csv('/data/sub4.csv', sep=',', index=False, columns=['reviewID','rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "tokenized_sent['sent'] = tokenized_sent['sent'].apply(laนอนผmbda k: [e.strip('\\n') for e in k ])\n",
    "tokenized_sent['sent'] = tokenized_sent['sent'].apply(lambda k: [e.strip(' ') for e in k ])\n",
    "tokenized_sent['sent'] = tokenized_sent['sent'].apply(lambda k: [e.strip('\\t')for e in k ])\n",
    "tokenized_sent['sent'] = tokenized_sent['sent'].apply(lambda k: [e.strip('\\!')for e in k ])\n",
    "tokenized_sent['sent'] = tokenized_sent['sent'].apply(lambda k: [e.strip('\\#')for e in k ])\n",
    "\n",
    "tokenized_sent['sent'] = tokenized_sent['sent'].apply(lambda k: [e.strip('\\n') for e in k ])\n",
    "tokenized_sent['sent'] = tokenized_sent['sent'].apply(lambda k: [e.strip(' ') for e in k ])\n",
    "tokenized_sent['sent'] = tokenized_sent['sent'].apply(lambda k: [e.strip('\\t')for e in k ])\n",
    "tokenized_sent['sent'] = tokenized_sent['sent'].apply(lambda k: [e.strip('\\!')for e in k ])\n",
    "tokenized_sent['sent'] = tokenized_sent['sent'].apply(lambda k: [e.strip('#')for e in k ])\n",
    "\n",
    "tokenized_sent['sent'] = tokenized_sent['sent'].apply(lambda k: [e.strip('\\n') for e in k ])\n",
    "tokenized_sent['sent'] = tokenized_sent['sent'].apply(lambda k: [e.strip(' ') for e in k ])\n",
    "tokenized_sent['sent'] = tokenized_sent['sent'].apply(lambda k: [e.strip('\\t')for e in k ])\n",
    "tokenized_sent['sent'] = tokenized_sent['sent'].apply(lambda k: [e.strip('\\!')for e in k ])\n",
    "tokenized_sent['sent_print (dict(zip(vectorizer.get_feature_names(), idf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##TF\n",
    "# # tokenize=['']\n",
    "# tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english')\n",
    "# tfs = tfidf.fit_transform(token_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Predict Model\n",
    "# import keras.backend as K\n",
    "# K.clear_session()\n",
    "def get_predict_model3():\n",
    "    input1 = Input(shape=(max_len,))\n",
    "    x = Embedding(len(dictionary)+1, 300, trainable=True)(input1)\n",
    "    x = Conv1D(32,5,strides=1,activation='relu',padding=\"valid\")(x)\n",
    "    x = MaxPooling1D(pool_size=5, strides=1, padding='valid')(x)\n",
    "    x = TimeDistributed(Dense(5))(x)\n",
    "    x = Flatten()(x)\n",
    "#     x = GRU(50, activation='relu')(x)\n",
    "#     x = Bidirectional(GRU(50 ,activation='relu'))(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Dense(50, activation='relu')(x)\n",
    "#     x = Dropout(0.25)(x)\n",
    "#     x = Dense(100, activation='relu')(x)\n",
    "    out = Dense(5, activation='softmax')(x)\n",
    "    model = Model(inputs=input1, outputs=out)\n",
    "    model.compile(optimizer=Adam(),\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['categorical_accuracy'])          \n",
    "    return model\n",
    "model3 = get_predict_model2()\n",
    "model3.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
