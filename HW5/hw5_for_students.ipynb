{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOMEWORK 5: TEXT CLASSIFICATION\n",
    "In this homework, you will create models to classify texts from TRUE call-center. There are two classification tasks:\n",
    "1. Action Classification: Identify which action the customer would like to take (e.g. enquire, report, cancle)\n",
    "2. Object Classification: Identify which object the customer is referring to (e.g. payment, truemoney, internet, roaming) \n",
    "\n",
    "In this homework, you are asked to do the following tasks:\n",
    "1. Data Cleaning\n",
    "2. Preprocessing data for keras\n",
    "3. Build and evaluate a model for \"action\" classification\n",
    "4. Build and evaluate a model for \"object\" classification\n",
    "5. Build and evaluate a multi-task model that does both \"action\" and \"object\" classifications in one-go \n",
    "\n",
    "\n",
    "Note: we have removed phone numbers from the dataset for privacy purposes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ekapolc/.env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Conv1D, Dropout, GRU, Bidirectional, Conv2D, LSTM\n",
    "from keras.layers import Reshape, Activation, Flatten, TimeDistributed,MaxPooling1D, MaxPooling2D\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers.merge import Dot\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "First, we load the data from disk into a Dataframe.\n",
    "\n",
    "A Dataframe is essentially a table, or 2D-array/Matrix with a name for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('clean-phone-data-for-students.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's preview the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence Utterance</th>\n",
       "      <th>Action</th>\n",
       "      <th>Object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;PHONE_NUMBER_REMOVED&gt; ผมไปจ่ายเงินที่ Counte...</td>\n",
       "      <td>enquire</td>\n",
       "      <td>payment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>internet ยังความเร็วอยุ่เท่าไหร ครับ</td>\n",
       "      <td>enquire</td>\n",
       "      <td>package</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ตะกี้ไปชำระค่าบริการไปแล้ว แต่ยังใช้งานไม่ได้...</td>\n",
       "      <td>report</td>\n",
       "      <td>suspend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>พี่ค่ะยังใช้ internet ไม่ได้เลยค่ะ เป็นเครื่อ...</td>\n",
       "      <td>enquire</td>\n",
       "      <td>internet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ฮาโหล คะ พอดีว่าเมื่อวานเปิดซิมทรูมูฟ แต่มันโ...</td>\n",
       "      <td>report</td>\n",
       "      <td>phone_issues</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Sentence Utterance   Action        Object\n",
       "0   <PHONE_NUMBER_REMOVED> ผมไปจ่ายเงินที่ Counte...  enquire       payment\n",
       "1               internet ยังความเร็วอยุ่เท่าไหร ครับ  enquire       package\n",
       "2   ตะกี้ไปชำระค่าบริการไปแล้ว แต่ยังใช้งานไม่ได้...   report       suspend\n",
       "3   พี่ค่ะยังใช้ internet ไม่ได้เลยค่ะ เป็นเครื่อ...  enquire      internet\n",
       "4   ฮาโหล คะ พอดีว่าเมื่อวานเปิดซิมทรูมูฟ แต่มันโ...   report  phone_issues"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence Utterance</th>\n",
       "      <th>Action</th>\n",
       "      <th>Object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16175</td>\n",
       "      <td>16175</td>\n",
       "      <td>16175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>13389</td>\n",
       "      <td>10</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>บริการอื่นๆ</td>\n",
       "      <td>enquire</td>\n",
       "      <td>service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>97</td>\n",
       "      <td>10377</td>\n",
       "      <td>2525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sentence Utterance   Action   Object\n",
       "count               16175    16175    16175\n",
       "unique              13389       10       33\n",
       "top           บริการอื่นๆ  enquire  service\n",
       "freq                   97    10377     2525"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the top 5 rows\n",
    "display(data_df.head())\n",
    "# Summarize the data\n",
    "data_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "We call the DataFrame.describe() again.\n",
    "Notice that there are 33 unique labels/classes for object and 10 unique labels for action that the model will try to predict.\n",
    "But there are unwanted duplications e.g. Idd,idd,lotalty_card,Lotalty_card\n",
    "\n",
    "Also note that, there are 13389 unqiue sentence utterances from 16175 utterances. You have to clean that too!\n",
    "\n",
    "## #TODO 1: \n",
    "You will have to remove unwanted label duplications as well as duplications in text inputs. \n",
    "Also, you will have to trim out unwanted whitespaces from the text inputs. \n",
    "This shouldn't be too hard, as you have already seen it in the demo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence Utterance</th>\n",
       "      <th>Action</th>\n",
       "      <th>Object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16175</td>\n",
       "      <td>16175</td>\n",
       "      <td>16175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>13389</td>\n",
       "      <td>10</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>บริการอื่นๆ</td>\n",
       "      <td>enquire</td>\n",
       "      <td>service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>97</td>\n",
       "      <td>10377</td>\n",
       "      <td>2525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sentence Utterance   Action   Object\n",
       "count               16175    16175    16175\n",
       "unique              13389       10       33\n",
       "top           บริการอื่นๆ  enquire  service\n",
       "freq                   97    10377     2525"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['payment', 'package', 'suspend', 'internet', 'phone_issues',\n",
       "       'service', 'nonTrueMove', 'balance', 'detail', 'bill', 'credit',\n",
       "       'promotion', 'mobile_setting', 'iservice', 'roaming', 'truemoney',\n",
       "       'information', 'lost_stolen', 'balance_minutes', 'idd',\n",
       "       'TrueMoney', 'garbage', 'Payment', 'IDD', 'ringtone', 'Idd',\n",
       "       'rate', 'loyalty_card', 'contact', 'officer', 'Balance', 'Service',\n",
       "       'Loyalty_card'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['enquire', 'report', 'cancel', 'Enquire', 'buy', 'activate',\n",
       "       'request', 'Report', 'garbage', 'change'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(data_df.describe())\n",
    "display(data_df.Object.unique())\n",
    "display(data_df.Action.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sent</th>\n",
       "      <th>Action</th>\n",
       "      <th>Object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16175</td>\n",
       "      <td>16175</td>\n",
       "      <td>16175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>13389</td>\n",
       "      <td>8</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>บริการอื่นๆ</td>\n",
       "      <td>enquire</td>\n",
       "      <td>service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>97</td>\n",
       "      <td>10484</td>\n",
       "      <td>2528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Sent   Action   Object\n",
       "count         16175    16175    16175\n",
       "unique        13389        8       26\n",
       "top     บริการอื่นๆ  enquire  service\n",
       "freq             97    10484     2528"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['enquire', 'report', 'cancel', 'buy', 'activate', 'request',\n",
       "       'garbage', 'change'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['payment', 'package', 'suspend', 'internet', 'phone_issues',\n",
       "       'service', 'nontruemove', 'balance', 'detail', 'bill', 'credit',\n",
       "       'promotion', 'mobile_setting', 'iservice', 'roaming', 'truemoney',\n",
       "       'information', 'lost_stolen', 'balance_minutes', 'idd', 'garbage',\n",
       "       'ringtone', 'rate', 'loyalty_card', 'contact', 'officer'],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO1: Data cleaning\n",
    "data_df['Action']=data_df['Action'].str.lower().copy()\n",
    "data_df['Object']=data_df['Object'].str.lower().copy()\n",
    "data_df = data_df.rename(index=str, columns={\"Sentence Utterance\": \"Sent\", \"Action\": \"Action\", \"Object\":\"Object\"})\n",
    "display(data_df.describe())\n",
    "display(data_df.Action.unique())\n",
    "display(data_df.Object.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "#data_df['Sent'] = data_df['Sent'].apply(lambda k : re.sub(r'[\"|–|\\'|:|;|?|$|!|~|\\n|\\t|-|#|+|<|>|/|\\\\|\\|{|}|\\[|\\]|`|0|1|2|3|4|5|6|7|8|9|*|.|%|@|$|^|&|=|:|(|)|-|_]', r'', k))\n",
    "data_df['Sent'] = data_df['Sent'].apply(lambda k : re.sub(r'[\"|–|\\'|:|;|?|$|!|~|\\n|\\t|-|#|+|<|>|/|\\\\|\\|{|}|\\[|\\]|`|*|%|@|$|^|&|=|:|(|)|-|_]', r'', k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sent</th>\n",
       "      <th>Action</th>\n",
       "      <th>Object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>13380</td>\n",
       "      <td>13380</td>\n",
       "      <td>13380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>13380</td>\n",
       "      <td>8</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ผมยากทราบว่า ผมใช้ package internet อะไรอยุ่ครับ</td>\n",
       "      <td>enquire</td>\n",
       "      <td>service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>8650</td>\n",
       "      <td>2111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Sent   Action   Object\n",
       "count                                              13380    13380    13380\n",
       "unique                                             13380        8       26\n",
       "top     ผมยากทราบว่า ผมใช้ package internet อะไรอยุ่ครับ  enquire  service\n",
       "freq                                                   1     8650     2111"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_df = data_df.drop_duplicates(\"Sent\", keep=\"first\")\n",
    "display(data_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #TODO 2: Preprocessing data for Keras\n",
    "You will be using Keras in this assignment. Please show us how you prepare your data for keras.\n",
    "Don't forget to split data into train and test sets (+ validation set if you want)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO2: Preprocessing data for Keras\n",
    "data_set = data_df.copy()\n",
    "data_set['Sent'] = data_df['Sent'].apply(lambda row: list(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a character map\n",
    "CHARS = [\n",
    "  '\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+',\n",
    "  ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8',\n",
    "  '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E',\n",
    "  'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R',\n",
    "  'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_',\n",
    "  'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
    "  'n', 'o', 'other', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y',\n",
    "  'z', '}', '~', 'ก', 'ข', 'ฃ', 'ค', 'ฅ', 'ฆ', 'ง', 'จ', 'ฉ', 'ช',\n",
    "  'ซ', 'ฌ', 'ญ', 'ฎ', 'ฏ', 'ฐ', 'ฑ', 'ฒ', 'ณ', 'ด', 'ต', 'ถ', 'ท',\n",
    "  'ธ', 'น', 'บ', 'ป', 'ผ', 'ฝ', 'พ', 'ฟ', 'ภ', 'ม', 'ย', 'ร', 'ฤ',\n",
    "  'ล', 'ว', 'ศ', 'ษ', 'ส', 'ห', 'ฬ', 'อ', 'ฮ', 'ฯ', 'ะ', 'ั', 'า',\n",
    "  'ำ', 'ิ', 'ี', 'ึ', 'ื', 'ุ', 'ู', 'ฺ', 'เ', 'แ', 'โ', 'ใ', 'ไ',\n",
    "  'ๅ', 'ๆ', '็', '่', '้', '๊', '๋', '์', 'ํ', '๐', '๑', '๒', '๓',\n",
    "  '๔', '๕', '๖', '๗', '๘', '๙', '‘', '’', '\\ufeff'\n",
    "]\n",
    "CHARS_MAP = {v: k for k, v in enumerate(CHARS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_n_gram_df(df, n_pad):\n",
    "    \"\"\"\n",
    "    Given an input dataframe, create a feature dataframe of shifted characters\n",
    "    Input:\n",
    "    df: timeseries of size (N)\n",
    "    n_pad: the number of context. For a given character at position [idx],\n",
    "    character at position [idx-n_pad/2 : idx+n_pad/2] will be used \n",
    "    as features for that character.\n",
    "\n",
    "    Output:\n",
    "    dataframe of size (N * n_pad) which each row contains the character, \n",
    "    n_pad_2 characters to the left, and n_pad_2 characters to the right\n",
    "    of that character.\n",
    "    \"\"\"\n",
    "    n_pad_2 = int((n_pad - 1)/2)\n",
    "    for i in range(n_pad_2):\n",
    "        df['char-{}'.format(i+1)] = df['char'].shift(i + 1)\n",
    "        df['char{}'.format(i+1)] = df['char'].shift(-i - 1)\n",
    "    return df[n_pad_2: -n_pad_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_feature(input_string):\n",
    "    \"\"\"\n",
    "    Transform the path to a directory containing processed files \n",
    "    into a feature matrix and output array\n",
    "    Input:\n",
    "    best_processed_path: str, path to a processed version of the BEST dataset\n",
    "    option: str, 'train' or 'test'\n",
    "    \"\"\"\n",
    "    # we use padding equals 21 here to consider 10 characters to the left\n",
    "    # and 10 characters to the right as features for the character in the middle\n",
    "    n_pad = 21\n",
    "    n_pad_2 = int((n_pad - 1)/2)\n",
    "    pad = [{'char' : ' '}]\n",
    "    df_pad = pd.DataFrame(pad * n_pad_2)\n",
    "\n",
    "#     df = pd.DataFrame(data=best_processed_path['sent'][0], columns=['char'])\n",
    "    df = pd.DataFrame(data=input_string, columns=['char'])\n",
    "    # pad with empty string feature\n",
    "    df = pd.concat((df_pad, df, df_pad))\n",
    "    \n",
    "    # map characters to numbers, use 'other' if not in the predefined character set.\n",
    "    df['char'] = df['char'].map(lambda x: CHARS_MAP.get(x, 80))\n",
    "    # Use nearby characters as features\n",
    "    df_with_context = create_n_gram_df(df, n_pad=n_pad)\n",
    "\n",
    "    char_row = ['char' + str(i + 1) for i in range(n_pad_2)] + \\\n",
    "             ['char-' + str(i + 1) for i in range(n_pad_2)] + ['char']\n",
    "\n",
    "    # convert pandas dataframe to numpy array to feed to the model\n",
    "    x_char = df_with_context[char_row].as_matrix()\n",
    "\n",
    "    return x_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenize model\n",
    "def get_my_tokenize_model():\n",
    "    input1 = Input(shape=(21,))\n",
    "    x = Embedding(178,8)(input1)\n",
    "    x = Conv1D(100,5,strides=1,activation='relu',padding=\"same\")(x)\n",
    "    x = TimeDistributed(Dense(5))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    out = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs=input1, outputs=out)\n",
    "    model.compile(optimizer=Adam(),\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['acc'])          \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## LOAD Tokenize model\n",
    "weight_path_model_best='/data/model_best.h5'\n",
    "\n",
    "tokenize_model = get_my_tokenize_model()\n",
    "tokenize_model.load_weights(weight_path_model_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_pred_to_word(y_pred,sent):\n",
    "    out = []\n",
    "    w = ''\n",
    "    for i in range(len(y_pred)):\n",
    "        if(y_pred[i] == 1):\n",
    "            out.append(w)\n",
    "            w = sent[i]\n",
    "        else:\n",
    "            w += sent[i]\n",
    "#         t = w.strip()\n",
    "#         if(t != ''):\n",
    "    out.append(w.strip())\n",
    "    return out[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13380/13380 [04:11<00:00, 53.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 33s, sys: 9.29 s, total: 4min 43s\n",
      "Wall time: 4min 11s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tqdm import tqdm\n",
    "\n",
    "tokenized_sent = []\n",
    "for i in tqdm(range(len(data_set))):\n",
    "    chars_array = prepare_feature(data_set['Sent'][i])\n",
    "    y_pred = tokenize_model.predict(chars_array)\n",
    "    #map probability to class\n",
    "    prob_to_class = lambda p: 1 if p[0]>=0.5 else 0\n",
    "    y_pred = np.apply_along_axis(prob_to_class,1,y_pred)\n",
    "    tokenized_sent.append(map_pred_to_word(y_pred,data_df['Sent'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prepared_data = pd.DataFrame(data={'Sent':np.array(tokenized_sent), 'Action':data_df['Action'].as_matrix(),\\\n",
    "                                  'Object':data_df['Object'].as_matrix()} , columns=['Sent','Action','Object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save tokenize\n",
    "with open('/data/tokenized-truevoice-clean', 'wb') as f:\n",
    "    pickle.dump(prepared_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load tokenize\n",
    "with open('/data/tokenized-truevoice-clean', 'rb') as f:\n",
    "    tokenized_sent = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## read fasttext\n",
    "ftext_w = {}\n",
    "with open('/data/fasttext/wiki.th.vec', 'r') as f:\n",
    "    embeded_w = f.readlines()\n",
    "for line in embeded_w:\n",
    "    values = line.rstrip().rsplit(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    ftext_w[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600 0.028908447977699196\n",
      "[(' ', 24314), ('ค่ะ', 5957), ('UNK', 5600), ('จะ', 5448), ('ครับ', 4628), ('ได้', 3885), ('ไม่', 3656), ('ใช้', 3388), ('ว่า', 2772), ('ผม', 2689)]\n"
     ]
    }
   ],
   "source": [
    "max_len = 55\n",
    "def create_index(input_text):\n",
    "    count_word = 0\n",
    "    words = []\n",
    "    \n",
    "    for sent in input_text:\n",
    "        for w in sent:\n",
    "            words.append(w.strip('\\n'))\n",
    "            count_word +=1\n",
    "    \n",
    "    word_count = list()\n",
    "    #use set and len to get the number of unique words\n",
    "    word_count.extend(collections.Counter(words).most_common(len(set(words))))\n",
    "    \n",
    "    #include a token for unknown word\n",
    "    threshold = 2\n",
    "    num_UNK = 0\n",
    "    index = len(word_count) - 1\n",
    "    rare_word = set()\n",
    "    \n",
    "    \n",
    "    while(word_count[index][1] <= threshold):\n",
    "        num_UNK += word_count[index][1]\n",
    "        rare_word.add(word_count[index][0])\n",
    "        index -= 1\n",
    "    \n",
    "    word_count = word_count[:index+1]\n",
    "    word_count.append((\"UNK\",num_UNK))\n",
    "    word_count = sorted(word_count, key=lambda x: -x[1])\n",
    "    \n",
    "    print(num_UNK , num_UNK/count_word)       \n",
    "\n",
    "    #print out 10 most frequent words\n",
    "    \n",
    "    print(word_count[:10])\n",
    "    dictionary = dict()\n",
    "    dictionary[\"for_keras_zero_padding\"] = 0\n",
    "    \n",
    "    for word in word_count:\n",
    "        dictionary[word[0]] = len(dictionary)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    \n",
    "    dataset = list()\n",
    "    for sent in input_text:\n",
    "        dataset.append([])\n",
    "        for word in sent[:max_len]:\n",
    "            if(word not in rare_word):\n",
    "                dataset[-1].append(dictionary[word])\n",
    "            else:\n",
    "                dataset[-1].append(dictionary[\"UNK\"])\n",
    "\n",
    "    return dataset, dictionary, reverse_dictionary\n",
    "\n",
    "dataset ,dictionary,reverse_dictionary = create_index(tokenized_sent['Sent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13380"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activate': 4,\n",
       " 'buy': 3,\n",
       " 'cancel': 2,\n",
       " 'change': 7,\n",
       " 'enquire': 0,\n",
       " 'garbage': 6,\n",
       " 'report': 1,\n",
       " 'request': 5}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_mapped = dict(zip(data_set['Action'].unique(),range(len(data_set['Action'].unique()))))\n",
    "action_mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'balance': 7,\n",
       " 'balance_minutes': 18,\n",
       " 'bill': 9,\n",
       " 'contact': 24,\n",
       " 'credit': 10,\n",
       " 'detail': 8,\n",
       " 'garbage': 20,\n",
       " 'idd': 19,\n",
       " 'information': 16,\n",
       " 'internet': 3,\n",
       " 'iservice': 13,\n",
       " 'lost_stolen': 17,\n",
       " 'loyalty_card': 23,\n",
       " 'mobile_setting': 12,\n",
       " 'nontruemove': 6,\n",
       " 'officer': 25,\n",
       " 'package': 1,\n",
       " 'payment': 0,\n",
       " 'phone_issues': 4,\n",
       " 'promotion': 11,\n",
       " 'rate': 22,\n",
       " 'ringtone': 21,\n",
       " 'roaming': 14,\n",
       " 'service': 5,\n",
       " 'suspend': 2,\n",
       " 'truemoney': 15}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object_mapped = dict(zip(data_set['Object'].unique(),range(len(data_set['Object'].unique()))))\n",
    "object_mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    1\n",
       "Name: Action, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set['Action'] = data_set['Action'].apply(lambda r : action_mapped[r])\n",
    "data_set['Action'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    1\n",
       "2    2\n",
       "Name: Object, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set['Object'] = data_set['Object'].apply(lambda r : object_mapped[r])\n",
    "data_set['Object'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11373 2007 2007 2007\n"
     ]
    }
   ],
   "source": [
    "X_train = sequence.pad_sequences(dataset, maxlen=max_len, padding='post', truncating='pre') #padding\n",
    "y_action_train = data_set['Action'].as_matrix()\n",
    "y_object_train = data_set['Object'].as_matrix()\n",
    "\n",
    "y_action_test = list(y_action_train[int(len(y_action_train)*0.85):])\n",
    "y_action_train = y_action_train[:int(len(y_action_train)*0.85)]\n",
    "\n",
    "y_object_test = list(y_object_train[int(len(y_object_train)*0.85):])\n",
    "y_object_train = y_object_train[:int(len(y_object_train)*0.85)]\n",
    "\n",
    "y_action_train = pd.get_dummies(y_action_train).as_matrix()\n",
    "y_object_train = pd.get_dummies(y_object_train).as_matrix()\n",
    "\n",
    "X_test = X_train[int(len(X_train)*0.85):]\n",
    "X_train = X_train[:int(len(X_train)*0.85)]\n",
    "\n",
    "print(len(X_train), len(X_test), len(y_action_test),len(y_object_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match 917 0.5094444444444445\n"
     ]
    }
   ],
   "source": [
    "match = 0\n",
    "for w in dictionary.keys():\n",
    "    if(w.strip('\\n') in ftext_w.keys()):\n",
    "        match += 1\n",
    "\n",
    "print('match', match, match/len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Prepare embed layer\n",
    "def prepare_embed(pretrain):\n",
    "    pre_emb = []\n",
    "    pre_emb.append(np.zeros(300))\n",
    "\n",
    "    for k in dictionary.keys():\n",
    "        if(not k in pretrain.keys()):    \n",
    "            pre_emb.append(np.zeros(300))\n",
    "        else:\n",
    "            if(len(pretrain[k]) ==  300):\n",
    "                pre_emb.append(pretrain[k])\n",
    "            else:\n",
    "                pre_emb.append(np.zeros(300))\n",
    "    return pre_emb\n",
    "\n",
    "pre_emb = np.array(prepare_embed(ftext_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score,precision_score,recall_score\n",
    "def evaluate(x_test, y_test, model):\n",
    "    \"\"\"\n",
    "    Evaluate model on the splitted 10 percent testing set.\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(x_test)\n",
    "    #map probability to class\n",
    "    y_pred_mapped = []\n",
    "    for i,pred in enumerate(y_pred):\n",
    "        pred = list(pred)\n",
    "        y_pred_mapped.append(pred.index(max(pred)))    \n",
    "    \n",
    "    f1score = f1_score(y_test,y_pred_mapped, average='weighted')\n",
    "    precision = precision_score(y_test,y_pred_mapped, average='weighted')\n",
    "    recall = recall_score(y_test,y_pred_mapped, average='weighted')\n",
    "    return f1score, precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #TODO 3: Build and evaluate a model for \"action\" classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_33 (InputLayer)        (None, 55)                0         \n",
      "_________________________________________________________________\n",
      "embedding_32 (Embedding)     (None, 55, 300)           540300    \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 55, 32)            48032     \n",
      "_________________________________________________________________\n",
      "dropout_66 (Dropout)         (None, 55, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 55, 32)            5152      \n",
      "_________________________________________________________________\n",
      "time_distributed_23 (TimeDis (None, 55, 5)             165       \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 275)               0         \n",
      "_________________________________________________________________\n",
      "dense_121 (Dense)            (None, 64)                17664     \n",
      "_________________________________________________________________\n",
      "dense_122 (Dense)            (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_67 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_123 (Dense)            (None, 8)                 520       \n",
      "=================================================================\n",
      "Total params: 615,993\n",
      "Trainable params: 615,993\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#TODO 3: Build and evaluate a model for \"action\" classification\n",
    "## Predict Model\n",
    "def get_predict_model():\n",
    "    input1 = Input(shape=(max_len,))\n",
    "    x = Embedding(len(dictionary)+1, 300, weights=[pre_emb], trainable=True)(input1)\n",
    "    x = Conv1D(32,5,strides=1,activation='relu',padding=\"same\")(x)\n",
    "#     x = MaxPooling1D(pool_size=5, strides=1, padding='same')(x)    \n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Conv1D(32,5,strides=1,activation='relu',padding=\"same\")(x)\n",
    "#     x = MaxPooling1D(pool_size=5, strides=1, padding='same')(x)   \n",
    "    x = TimeDistributed(Dense(5))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    out = Dense(8, activation='softmax')(x)\n",
    "    model = Model(inputs=input1, outputs=out)\n",
    "    model.compile(optimizer=Adam(),\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['categorical_accuracy'])          \n",
    "    return model\n",
    "model = get_predict_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9098 samples, validate on 2275 samples\n",
      "Epoch 1/10\n",
      "8704/9098 [===========================>..] - ETA: 0s - loss: 1.5259 - categorical_accuracy: 0.5638Epoch 00000: val_loss improved from inf to 1.04897, saving model to /data/truevoice-action.h5\n",
      "9098/9098 [==============================] - 4s - loss: 1.5134 - categorical_accuracy: 0.5672 - val_loss: 1.0490 - val_categorical_accuracy: 0.7257\n",
      "Epoch 2/10\n",
      "8448/9098 [==========================>...] - ETA: 0s - loss: 1.2048 - categorical_accuracy: 0.6085Epoch 00001: val_loss improved from 1.04897 to 0.89433, saving model to /data/truevoice-action.h5\n",
      "9098/9098 [==============================] - 0s - loss: 1.1958 - categorical_accuracy: 0.6101 - val_loss: 0.8943 - val_categorical_accuracy: 0.7257\n",
      "Epoch 3/10\n",
      "8192/9098 [==========================>...] - ETA: 0s - loss: 1.0540 - categorical_accuracy: 0.6416Epoch 00002: val_loss improved from 0.89433 to 0.79453, saving model to /data/truevoice-action.h5\n",
      "9098/9098 [==============================] - 0s - loss: 1.0446 - categorical_accuracy: 0.6452 - val_loss: 0.7945 - val_categorical_accuracy: 0.7613\n",
      "Epoch 4/10\n",
      "8448/9098 [==========================>...] - ETA: 0s - loss: 0.8820 - categorical_accuracy: 0.6986Epoch 00003: val_loss improved from 0.79453 to 0.72369, saving model to /data/truevoice-action.h5\n",
      "9098/9098 [==============================] - 0s - loss: 0.8754 - categorical_accuracy: 0.7017 - val_loss: 0.7237 - val_categorical_accuracy: 0.7789\n",
      "Epoch 5/10\n",
      "8704/9098 [===========================>..] - ETA: 0s - loss: 0.7361 - categorical_accuracy: 0.7582Epoch 00004: val_loss improved from 0.72369 to 0.65498, saving model to /data/truevoice-action.h5\n",
      "9098/9098 [==============================] - 0s - loss: 0.7345 - categorical_accuracy: 0.7588 - val_loss: 0.6550 - val_categorical_accuracy: 0.7943\n",
      "Epoch 6/10\n",
      "8448/9098 [==========================>...] - ETA: 0s - loss: 0.5964 - categorical_accuracy: 0.8073Epoch 00005: val_loss improved from 0.65498 to 0.56684, saving model to /data/truevoice-action.h5\n",
      "9098/9098 [==============================] - 0s - loss: 0.5980 - categorical_accuracy: 0.8073 - val_loss: 0.5668 - val_categorical_accuracy: 0.8308\n",
      "Epoch 7/10\n",
      "8448/9098 [==========================>...] - ETA: 0s - loss: 0.4866 - categorical_accuracy: 0.8500Epoch 00006: val_loss did not improve\n",
      "9098/9098 [==============================] - 0s - loss: 0.4874 - categorical_accuracy: 0.8503 - val_loss: 0.6106 - val_categorical_accuracy: 0.8356\n",
      "Epoch 8/10\n",
      "8448/9098 [==========================>...] - ETA: 0s - loss: 0.4109 - categorical_accuracy: 0.8707Epoch 00007: val_loss did not improve\n",
      "9098/9098 [==============================] - 0s - loss: 0.4129 - categorical_accuracy: 0.8696 - val_loss: 0.6088 - val_categorical_accuracy: 0.8286\n",
      "Epoch 9/10\n",
      "8448/9098 [==========================>...] - ETA: 0s - loss: 0.3580 - categorical_accuracy: 0.8846Epoch 00008: val_loss did not improve\n",
      "9098/9098 [==============================] - 0s - loss: 0.3593 - categorical_accuracy: 0.8838 - val_loss: 0.6314 - val_categorical_accuracy: 0.8281\n",
      "Epoch 10/10\n",
      "8704/9098 [===========================>..] - ETA: 0s - loss: 0.3108 - categorical_accuracy: 0.8989Epoch 00009: val_loss did not improve\n",
      "9098/9098 [==============================] - 0s - loss: 0.3114 - categorical_accuracy: 0.8985 - val_loss: 0.6121 - val_categorical_accuracy: 0.8325\n",
      "CPU times: user 11 s, sys: 1.54 s, total: 12.6 s\n",
      "Wall time: 11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "weight_path_model_best='/data/truevoice-action.h5'\n",
    "\n",
    "callbacks_list = [\n",
    "#    TensorBoard(log_dir='/data/Graph/midterm', histogram_freq=1, write_grads=True),\n",
    "    ModelCheckpoint(\n",
    "        weight_path_model_best,\n",
    "        monitor = \"val_loss\",\n",
    "        mode = 'min',\n",
    "        verbose = 1,\n",
    "        save_best_only = True,\n",
    "        save_weights_only = True,\n",
    "    ),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                    patience=2, min_lr=0.001)\n",
    "]\n",
    "\n",
    "model.fit(X_train,y_action_train,batch_size=256,epochs=10,verbose=1, validation_split=0.2, shuffle=True, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ekapolc/.env/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ekapolc/.env/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8041230511277234, 0.786206807495494, 0.8256103637269556)"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(X_test, y_action_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #TODO 4: Build and evaluate a model for \"object\" classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 55)                0         \n",
      "_________________________________________________________________\n",
      "embedding_6 (Embedding)      (None, 55, 300)           540300    \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 55, 100)           150100    \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 55, 100)           50100     \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 55, 100)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 55, 100)           10100     \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 200)               120600    \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 26)                2626      \n",
      "=================================================================\n",
      "Total params: 914,126\n",
      "Trainable params: 914,126\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#TODO 4: Build and evaluate a model for \"object\" classification\n",
    "## Predict Model\n",
    "def get_object_model():\n",
    "    input1 = Input(shape=(max_len,))\n",
    "    x = Embedding(len(dictionary)+1, 300, weights=[pre_emb], trainable=True)(input1)\n",
    "    x = Conv1D(100,5,strides=1,activation='relu',padding=\"same\")(x)\n",
    "    x = Conv1D(100,5,strides=1,activation='relu',padding=\"same\")(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = TimeDistributed(Dense(100))(x)\n",
    "    x = Bidirectional(GRU(100))(x)\n",
    "#     x = Flatten()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    x = Dense(100, activation='relu')(x)    \n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    out = Dense(26, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input1, outputs=out)\n",
    "    model.compile(optimizer=Adam(),\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['categorical_accuracy'])          \n",
    "    return model\n",
    "model_2 = get_object_model()\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weight_path_model_obj='/data/truevoice-object.h5'\n",
    "\n",
    "obj_callbacks_list = [\n",
    "#    TensorBoard(log_dir='/data/Graph/midterm', histogram_freq=1, write_grads=True),\n",
    "    ModelCheckpoint(\n",
    "        weight_path_model_obj,\n",
    "        monitor = \"val_loss\",\n",
    "        mode = 'min',\n",
    "        verbose = 1,\n",
    "        save_best_only = True,\n",
    "        save_weights_only = True,\n",
    "    ),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                    patience=2, min_lr=0.001)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9667 samples, validate on 1706 samples\n",
      "Epoch 1/10\n",
      "9472/9667 [============================>.] - ETA: 0s - loss: 2.8222 - categorical_accuracy: 0.1500Epoch 00000: val_loss improved from inf to 2.57291, saving model to /data/truevoice-object.h5\n",
      "9667/9667 [==============================] - 7s - loss: 2.8182 - categorical_accuracy: 0.1504 - val_loss: 2.5729 - val_categorical_accuracy: 0.1260\n",
      "Epoch 2/10\n",
      "9472/9667 [============================>.] - ETA: 0s - loss: 2.4654 - categorical_accuracy: 0.2369Epoch 00001: val_loss improved from 2.57291 to 2.18640, saving model to /data/truevoice-object.h5\n",
      "9667/9667 [==============================] - 6s - loss: 2.4578 - categorical_accuracy: 0.2397 - val_loss: 2.1864 - val_categorical_accuracy: 0.3798\n",
      "Epoch 3/10\n",
      "9472/9667 [============================>.] - ETA: 0s - loss: 1.9985 - categorical_accuracy: 0.4150Epoch 00002: val_loss improved from 2.18640 to 1.73229, saving model to /data/truevoice-object.h5\n",
      "9667/9667 [==============================] - 6s - loss: 1.9964 - categorical_accuracy: 0.4156 - val_loss: 1.7323 - val_categorical_accuracy: 0.4683\n",
      "Epoch 4/10\n",
      "9472/9667 [============================>.] - ETA: 0s - loss: 1.6196 - categorical_accuracy: 0.5126Epoch 00003: val_loss improved from 1.73229 to 1.51021, saving model to /data/truevoice-object.h5\n",
      "9667/9667 [==============================] - 6s - loss: 1.6179 - categorical_accuracy: 0.5130 - val_loss: 1.5102 - val_categorical_accuracy: 0.5199\n",
      "Epoch 5/10\n",
      "9472/9667 [============================>.] - ETA: 0s - loss: 1.3812 - categorical_accuracy: 0.5762Epoch 00004: val_loss improved from 1.51021 to 1.42211, saving model to /data/truevoice-object.h5\n",
      "9667/9667 [==============================] - 6s - loss: 1.3824 - categorical_accuracy: 0.5753 - val_loss: 1.4221 - val_categorical_accuracy: 0.5574\n",
      "Epoch 6/10\n",
      "9472/9667 [============================>.] - ETA: 0s - loss: 1.1888 - categorical_accuracy: 0.6355Epoch 00005: val_loss improved from 1.42211 to 1.33119, saving model to /data/truevoice-object.h5\n",
      "9667/9667 [==============================] - 6s - loss: 1.1885 - categorical_accuracy: 0.6355 - val_loss: 1.3312 - val_categorical_accuracy: 0.5932\n",
      "Epoch 7/10\n",
      "9472/9667 [============================>.] - ETA: 0s - loss: 1.0462 - categorical_accuracy: 0.6765Epoch 00006: val_loss improved from 1.33119 to 1.27372, saving model to /data/truevoice-object.h5\n",
      "9667/9667 [==============================] - 6s - loss: 1.0497 - categorical_accuracy: 0.6759 - val_loss: 1.2737 - val_categorical_accuracy: 0.6114\n",
      "Epoch 8/10\n",
      "9472/9667 [============================>.] - ETA: 0s - loss: 0.9166 - categorical_accuracy: 0.7199Epoch 00007: val_loss improved from 1.27372 to 1.26359, saving model to /data/truevoice-object.h5\n",
      "9667/9667 [==============================] - 5s - loss: 0.9189 - categorical_accuracy: 0.7194 - val_loss: 1.2636 - val_categorical_accuracy: 0.6225\n",
      "Epoch 9/10\n",
      "9472/9667 [============================>.] - ETA: 0s - loss: 0.8260 - categorical_accuracy: 0.7545Epoch 00008: val_loss did not improve\n",
      "9667/9667 [==============================] - 6s - loss: 0.8295 - categorical_accuracy: 0.7532 - val_loss: 1.2908 - val_categorical_accuracy: 0.6184\n",
      "Epoch 10/10\n",
      "9472/9667 [============================>.] - ETA: 0s - loss: 0.7314 - categorical_accuracy: 0.7823Epoch 00009: val_loss did not improve\n",
      "9667/9667 [==============================] - 6s - loss: 0.7313 - categorical_accuracy: 0.7827 - val_loss: 1.3199 - val_categorical_accuracy: 0.6202\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1b3e0659b0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.fit(X_train,y_object_train,batch_size=256,epochs=10,verbose=1, validation_split=0.15, shuffle=True, callbacks=obj_callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_2.load_weights(weight_path_model_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ekapolc/.env/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ekapolc/.env/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6188333780285895, 0.626024551613759, 0.6347782760338814)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(X_test, y_object_test, model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #TODO 5: Build and evaluate a multi-task model that does both \"action\" and \"object\" classifications in one-go \n",
    "\n",
    "This can be a bit tricky, if you are not familiar with the Keras functional API. PLEASE READ this webpage(https://keras.io/getting-started/functional-api-guide/) before you start this task.   \n",
    "\n",
    "Your model will have 2 separate output layers one for action classification task and another for object classification task. \n",
    "\n",
    "This is a rough sketch of what your model might look like:\n",
    "![image](https://raw.githubusercontent.com/ekapolc/nlp_course/master/HW5/multitask_sketch.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_30 (InputLayer)            (None, 55)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_30 (Embedding)         (None, 55, 300)       540300      input_30[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_35 (Bidirectional) (None, 55, 200)       320800      embedding_30[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_74 (Dropout)             (None, 55, 200)       0           bidirectional_35[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_36 (Bidirectional) (None, 200)           240800      dropout_74[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_75 (Dropout)             (None, 200)           0           bidirectional_36[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dense_125 (Dense)                (None, 100)           20100       dropout_75[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_126 (Dense)                (None, 100)           10100       dense_125[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_127 (Dense)                (None, 100)           10100       dense_126[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_76 (Dropout)             (None, 100)           0           dense_127[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_128 (Dense)                (None, 8)             808         dropout_76[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_129 (Dense)                (None, 26)            2626        dropout_76[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 1,145,634\n",
      "Trainable params: 1,145,634\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#TODO 5: Build and evaluate a multi-task model that does both \"action\" and \"object\" classifications in one-go\n",
    "## Predict Model\n",
    "def get_predict_model_3():\n",
    "    input1 = Input(shape=(max_len,))\n",
    "    x = Embedding(len(dictionary)+1, 300, weights=[pre_emb], trainable=True)(input1)\n",
    "#     x = Conv1D(100,5,strides=1,activation='relu',padding=\"same\")(x)\n",
    "#     x = MaxPooling1D(pool_size=5, strides=1, padding='same')(x)   \n",
    "#     x = TimeDistributed(Dense(100))(x)\n",
    "#     x = Dropout(0.25)(x)\n",
    "    x = Bidirectional(LSTM(100,return_sequences=True))(x)    \n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Bidirectional(LSTM(100))(x)\n",
    "#     x = Flatten()(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    out1 = Dense(8, activation='softmax')(x)\n",
    "    out2 = Dense(26, activation='softmax')(x)\n",
    "    model = Model(inputs=input1, outputs=[out1,out2])\n",
    "    model.compile(optimizer=Adam(),\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['categorical_accuracy'])          \n",
    "    return model\n",
    "model_3 = get_predict_model_3()\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_path_model_obj='/data/truevoice-both.h5'\n",
    "\n",
    "both_callbacks_list = [\n",
    "#    TensorBoard(log_dir='/data/Graph/midterm', histogram_freq=1, write_grads=True),\n",
    "    ModelCheckpoint(\n",
    "        weight_path_model_obj,\n",
    "        monitor = \"val_loss\",\n",
    "        mode = 'min',\n",
    "        verbose = 1,\n",
    "        save_best_only = True,\n",
    "        save_weights_only = True,\n",
    "    ),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                    patience=2, min_lr=0.001)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9098 samples, validate on 2275 samples\n",
      "Epoch 1/15\n",
      "8960/9098 [============================>.] - ETA: 0s - loss: 4.4789 - dense_128_loss: 1.5231 - dense_129_loss: 2.9558 - dense_128_categorical_accuracy: 0.5798 - dense_129_categorical_accuracy: 0.1406Epoch 00000: val_loss improved from inf to 3.74009, saving model to /data/truevoice-both.h5\n",
      "9098/9098 [==============================] - 17s - loss: 4.4712 - dense_128_loss: 1.5198 - dense_129_loss: 2.9513 - dense_128_categorical_accuracy: 0.5805 - dense_129_categorical_accuracy: 0.1418 - val_loss: 3.7401 - val_dense_128_loss: 1.0672 - val_dense_129_loss: 2.6729 - val_dense_128_categorical_accuracy: 0.7257 - val_dense_129_categorical_accuracy: 0.1174\n",
      "Epoch 2/15\n",
      "8960/9098 [============================>.] - ETA: 0s - loss: 3.8985 - dense_128_loss: 1.2639 - dense_129_loss: 2.6346 - dense_128_categorical_accuracy: 0.6095 - dense_129_categorical_accuracy: 0.1773Epoch 00001: val_loss improved from 3.74009 to 3.28156, saving model to /data/truevoice-both.h5\n",
      "9098/9098 [==============================] - 13s - loss: 3.8946 - dense_128_loss: 1.2632 - dense_129_loss: 2.6313 - dense_128_categorical_accuracy: 0.6093 - dense_129_categorical_accuracy: 0.1783 - val_loss: 3.2816 - val_dense_128_loss: 0.9130 - val_dense_129_loss: 2.3685 - val_dense_128_categorical_accuracy: 0.7415 - val_dense_129_categorical_accuracy: 0.2985\n",
      "Epoch 3/15\n",
      "8960/9098 [============================>.] - ETA: 0s - loss: 3.1976 - dense_128_loss: 0.9615 - dense_129_loss: 2.2361 - dense_128_categorical_accuracy: 0.6724 - dense_129_categorical_accuracy: 0.3152Epoch 00002: val_loss improved from 3.28156 to 2.67166, saving model to /data/truevoice-both.h5\n",
      "9098/9098 [==============================] - 13s - loss: 3.1894 - dense_128_loss: 0.9573 - dense_129_loss: 2.2321 - dense_128_categorical_accuracy: 0.6736 - dense_129_categorical_accuracy: 0.3164 - val_loss: 2.6717 - val_dense_128_loss: 0.6890 - val_dense_129_loss: 1.9827 - val_dense_128_categorical_accuracy: 0.7969 - val_dense_129_categorical_accuracy: 0.4299\n",
      "Epoch 4/15\n",
      "8960/9098 [============================>.] - ETA: 0s - loss: 2.6654 - dense_128_loss: 0.7490 - dense_129_loss: 1.9164 - dense_128_categorical_accuracy: 0.7561 - dense_129_categorical_accuracy: 0.4415Epoch 00003: val_loss improved from 2.67166 to 2.38602, saving model to /data/truevoice-both.h5\n",
      "9098/9098 [==============================] - 13s - loss: 2.6599 - dense_128_loss: 0.7473 - dense_129_loss: 1.9126 - dense_128_categorical_accuracy: 0.7563 - dense_129_categorical_accuracy: 0.4428 - val_loss: 2.3860 - val_dense_128_loss: 0.6064 - val_dense_129_loss: 1.7797 - val_dense_128_categorical_accuracy: 0.8110 - val_dense_129_categorical_accuracy: 0.4840\n",
      "Epoch 5/15\n",
      "8960/9098 [============================>.] - ETA: 0s - loss: 2.3583 - dense_128_loss: 0.6501 - dense_129_loss: 1.7082 - dense_128_categorical_accuracy: 0.7872 - dense_129_categorical_accuracy: 0.4944Epoch 00004: val_loss improved from 2.38602 to 2.33755, saving model to /data/truevoice-both.h5\n",
      "9098/9098 [==============================] - 13s - loss: 2.3581 - dense_128_loss: 0.6498 - dense_129_loss: 1.7083 - dense_128_categorical_accuracy: 0.7868 - dense_129_categorical_accuracy: 0.4951 - val_loss: 2.3376 - val_dense_128_loss: 0.5801 - val_dense_129_loss: 1.7575 - val_dense_128_categorical_accuracy: 0.8273 - val_dense_129_categorical_accuracy: 0.4914\n",
      "Epoch 6/15\n",
      "8960/9098 [============================>.] - ETA: 0s - loss: 2.1729 - dense_128_loss: 0.5980 - dense_129_loss: 1.5749 - dense_128_categorical_accuracy: 0.8089 - dense_129_categorical_accuracy: 0.5382Epoch 00005: val_loss improved from 2.33755 to 2.22651, saving model to /data/truevoice-both.h5\n",
      "9098/9098 [==============================] - 13s - loss: 2.1690 - dense_128_loss: 0.5977 - dense_129_loss: 1.5713 - dense_128_categorical_accuracy: 0.8090 - dense_129_categorical_accuracy: 0.5401 - val_loss: 2.2265 - val_dense_128_loss: 0.5880 - val_dense_129_loss: 1.6385 - val_dense_128_categorical_accuracy: 0.8277 - val_dense_129_categorical_accuracy: 0.5196\n",
      "Epoch 7/15\n",
      "8960/9098 [============================>.] - ETA: 0s - loss: 2.0039 - dense_128_loss: 0.5450 - dense_129_loss: 1.4588 - dense_128_categorical_accuracy: 0.8288 - dense_129_categorical_accuracy: 0.5634Epoch 00006: val_loss improved from 2.22651 to 2.12021, saving model to /data/truevoice-both.h5\n",
      "9098/9098 [==============================] - 13s - loss: 2.0041 - dense_128_loss: 0.5452 - dense_129_loss: 1.4588 - dense_128_categorical_accuracy: 0.8285 - dense_129_categorical_accuracy: 0.5639 - val_loss: 2.1202 - val_dense_128_loss: 0.5574 - val_dense_129_loss: 1.5628 - val_dense_128_categorical_accuracy: 0.8356 - val_dense_129_categorical_accuracy: 0.5411\n",
      "Epoch 8/15\n",
      "8960/9098 [============================>.] - ETA: 0s - loss: 1.8345 - dense_128_loss: 0.5018 - dense_129_loss: 1.3327 - dense_128_categorical_accuracy: 0.8421 - dense_129_categorical_accuracy: 0.5977Epoch 00007: val_loss improved from 2.12021 to 2.11140, saving model to /data/truevoice-both.h5\n",
      "9098/9098 [==============================] - 13s - loss: 1.8302 - dense_128_loss: 0.4998 - dense_129_loss: 1.3304 - dense_128_categorical_accuracy: 0.8423 - dense_129_categorical_accuracy: 0.5971 - val_loss: 2.1114 - val_dense_128_loss: 0.5693 - val_dense_129_loss: 1.5421 - val_dense_128_categorical_accuracy: 0.8325 - val_dense_129_categorical_accuracy: 0.5556\n",
      "Epoch 9/15\n",
      "8960/9098 [============================>.] - ETA: 0s - loss: 1.6852 - dense_128_loss: 0.4558 - dense_129_loss: 1.2294 - dense_128_categorical_accuracy: 0.8579 - dense_129_categorical_accuracy: 0.6310Epoch 00008: val_loss improved from 2.11140 to 1.97324, saving model to /data/truevoice-both.h5\n",
      "9098/9098 [==============================] - 13s - loss: 1.6821 - dense_128_loss: 0.4539 - dense_129_loss: 1.2282 - dense_128_categorical_accuracy: 0.8585 - dense_129_categorical_accuracy: 0.6308 - val_loss: 1.9732 - val_dense_128_loss: 0.5240 - val_dense_129_loss: 1.4493 - val_dense_128_categorical_accuracy: 0.8475 - val_dense_129_categorical_accuracy: 0.5688\n",
      "Epoch 10/15\n",
      "8960/9098 [============================>.] - ETA: 0s - loss: 1.5463 - dense_128_loss: 0.4195 - dense_129_loss: 1.1267 - dense_128_categorical_accuracy: 0.8641 - dense_129_categorical_accuracy: 0.6600Epoch 00009: val_loss improved from 1.97324 to 1.91158, saving model to /data/truevoice-both.h5\n",
      "9098/9098 [==============================] - 13s - loss: 1.5482 - dense_128_loss: 0.4208 - dense_129_loss: 1.1274 - dense_128_categorical_accuracy: 0.8637 - dense_129_categorical_accuracy: 0.6595 - val_loss: 1.9116 - val_dense_128_loss: 0.5144 - val_dense_129_loss: 1.3972 - val_dense_128_categorical_accuracy: 0.8440 - val_dense_129_categorical_accuracy: 0.5820\n",
      "Epoch 11/15\n",
      "8960/9098 [============================>.] - ETA: 0s - loss: 1.4394 - dense_128_loss: 0.3900 - dense_129_loss: 1.0494 - dense_128_categorical_accuracy: 0.8778 - dense_129_categorical_accuracy: 0.6805Epoch 00010: val_loss did not improve\n",
      "9098/9098 [==============================] - 13s - loss: 1.4395 - dense_128_loss: 0.3906 - dense_129_loss: 1.0489 - dense_128_categorical_accuracy: 0.8778 - dense_129_categorical_accuracy: 0.6806 - val_loss: 1.9920 - val_dense_128_loss: 0.5455 - val_dense_129_loss: 1.4465 - val_dense_128_categorical_accuracy: 0.8338 - val_dense_129_categorical_accuracy: 0.5868\n",
      "Epoch 12/15\n",
      "8960/9098 [============================>.] - ETA: 0s - loss: 1.3436 - dense_128_loss: 0.3610 - dense_129_loss: 0.9827 - dense_128_categorical_accuracy: 0.8864 - dense_129_categorical_accuracy: 0.7046Epoch 00011: val_loss did not improve\n",
      "9098/9098 [==============================] - 13s - loss: 1.3429 - dense_128_loss: 0.3606 - dense_129_loss: 0.9822 - dense_128_categorical_accuracy: 0.8871 - dense_129_categorical_accuracy: 0.7048 - val_loss: 1.9310 - val_dense_128_loss: 0.5269 - val_dense_129_loss: 1.4041 - val_dense_128_categorical_accuracy: 0.8536 - val_dense_129_categorical_accuracy: 0.5930\n",
      "Epoch 13/15\n",
      "8960/9098 [============================>.] - ETA: 0s - loss: 1.2829 - dense_128_loss: 0.3489 - dense_129_loss: 0.9340 - dense_128_categorical_accuracy: 0.8893 - dense_129_categorical_accuracy: 0.7184Epoch 00012: val_loss improved from 1.91158 to 1.90188, saving model to /data/truevoice-both.h5\n",
      "9098/9098 [==============================] - 13s - loss: 1.2803 - dense_128_loss: 0.3487 - dense_129_loss: 0.9316 - dense_128_categorical_accuracy: 0.8891 - dense_129_categorical_accuracy: 0.7192 - val_loss: 1.9019 - val_dense_128_loss: 0.5332 - val_dense_129_loss: 1.3687 - val_dense_128_categorical_accuracy: 0.8453 - val_dense_129_categorical_accuracy: 0.6088\n",
      "Epoch 14/15\n",
      "8960/9098 [============================>.] - ETA: 0s - loss: 1.1971 - dense_128_loss: 0.3266 - dense_129_loss: 0.8705 - dense_128_categorical_accuracy: 0.8930 - dense_129_categorical_accuracy: 0.7400Epoch 00013: val_loss did not improve\n",
      "9098/9098 [==============================] - 13s - loss: 1.1954 - dense_128_loss: 0.3249 - dense_129_loss: 0.8706 - dense_128_categorical_accuracy: 0.8935 - dense_129_categorical_accuracy: 0.7405 - val_loss: 2.0484 - val_dense_128_loss: 0.5836 - val_dense_129_loss: 1.4647 - val_dense_128_categorical_accuracy: 0.8426 - val_dense_129_categorical_accuracy: 0.5903\n",
      "Epoch 15/15\n",
      "8960/9098 [============================>.] - ETA: 0s - loss: 1.1013 - dense_128_loss: 0.2991 - dense_129_loss: 0.8022 - dense_128_categorical_accuracy: 0.9022 - dense_129_categorical_accuracy: 0.7589Epoch 00014: val_loss did not improve\n",
      "9098/9098 [==============================] - 13s - loss: 1.1057 - dense_128_loss: 0.3014 - dense_129_loss: 0.8043 - dense_128_categorical_accuracy: 0.9014 - dense_129_categorical_accuracy: 0.7587 - val_loss: 2.0440 - val_dense_128_loss: 0.6256 - val_dense_129_loss: 1.4184 - val_dense_128_categorical_accuracy: 0.8418 - val_dense_129_categorical_accuracy: 0.5982\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1ae45952e8>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3.fit(X_train,[y_action_train,y_object_train],batch_size=256,epochs=15,verbose=1, validation_split=0.2, shuffle=True, callbacks=both_callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score,precision_score,recall_score\n",
    "def evaluate2(x_test, y_test, y_test2, model):\n",
    "    \"\"\"\n",
    "    Evaluate model on the splitted 10 percent testing set.\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(x_test)\n",
    "    \n",
    "    #map probability to class\n",
    "    y_pred_obj_mapped = []\n",
    "    y_pred_act_mapped = []\n",
    "    \n",
    "    for pred_act in y_pred[0]:\n",
    "        pred_act = list(pred_act)\n",
    "        y_pred_act_mapped.append(pred_act.index(max(pred_act)))\n",
    "    \n",
    "    for pred_obj in y_pred[1]:\n",
    "        pred_obj = list(pred_obj)\n",
    "        y_pred_obj_mapped.append(pred_obj.index(max(pred_obj)))\n",
    "    \n",
    "    f1score = f1_score(y_test,y_pred_act_mapped, average='weighted')\n",
    "    precision = precision_score(y_test,y_pred_act_mapped, average='weighted')\n",
    "    recall = recall_score(y_test,y_pred_act_mapped, average='weighted')\n",
    "    \n",
    "    f1score2 = f1_score(y_test2,y_pred_obj_mapped, average='weighted')\n",
    "    precision2 = precision_score(y_test2,y_pred_obj_mapped, average='weighted')\n",
    "    recall2 = recall_score(y_test2,y_pred_obj_mapped, average='weighted')\n",
    "    print(\"act {}, {}, {}\\nobj {}, {}, {}\\n\".format(f1score,precision,recall,f1score2,precision2,recall2))\n",
    "    acc = 0\n",
    "    for i in range(len(x_test)):\n",
    "        if( y_pred_act_mapped[i] == y_test[i] and y_pred_obj_mapped[i] == y_test2[i]):\n",
    "            acc += 1\n",
    "    print(\"acc: {}\".format(acc/len(x_test)))\n",
    "#     return f1score, precision, recall,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_3.load_weights(weight_path_model_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "act 0.8162753041497557, 0.7953877815318967, 0.8390632785251619\n",
      "obj 0.5750871407405673, 0.6177382904499122, 0.5959142999501744\n",
      "\n",
      "acc: 0.5705032386646737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ekapolc/.env/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ekapolc/.env/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "evaluate2(X_test, y_action_test, y_object_test, model_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
